#!/usr/bin/env python3
import os
import sqlite3
import pickle
import numpy as np
from typing import List, Dict, Any
import boto3
import json

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("âš ï¸ FAISSê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install faiss-cpu ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

class RAGSystem:
    def __init__(self, data_path="../data"):
        self.data_path = data_path
        self.db_path = os.path.join(data_path, "medicines.db")
        self.faiss_index_path = os.path.join(data_path, "mfds_faiss_index", "index.faiss")
        self.faiss_pkl_path = os.path.join(data_path, "mfds_faiss_index", "index.pkl")
        self.clean_pkl_path = os.path.join(data_path, "mfds_faiss_index", "index_clean.pkl")  # ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„°
        
        # AWS Bedrock í´ë¼ì´ì–¸íŠ¸
        self.session = boto3.Session()
        self.bedrock = self.session.client(service_name='bedrock-runtime', region_name='us-east-1')
        
        # FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.index = None
        self.metadata = None
        self._load_faiss_index()
        
    def _load_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if not FAISS_AVAILABLE:
            print("âš ï¸ FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ RAG ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
            return
            
        try:
            if os.path.exists(self.faiss_index_path):
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                self.index = faiss.read_index(self.faiss_index_path)
                print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index.ntotal}ê°œ ë¬¸ì„œ")
                
                # ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ìš°ì„  ì‹œë„
                if os.path.exists(self.clean_pkl_path):
                    try:
                        with open(self.clean_pkl_path, 'rb') as f:
                            self.metadata = pickle.load(f)
                        print(f"âœ… ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê°œ í•­ëª©")
                        return
                    except Exception as clean_error:
                        print(f"âš ï¸ ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {clean_error}")
                
                # ì›ë³¸ ë©”íƒ€ë°ì´í„° ì‹œë„ (langchain ì˜ì¡´ì„± í•„ìš”)
                if os.path.exists(self.faiss_pkl_path):
                    try:
                        with open(self.faiss_pkl_path, 'rb') as f:
                            original_data = pickle.load(f)
                        
                        # langchain docstoreì—ì„œ ë°ì´í„° ì¶”ì¶œ
                        if hasattr(original_data, '__len__') and len(original_data) >= 1:
                            docstore = original_data[0]
                            if hasattr(docstore, '_dict'):
                                documents = docstore._dict
                                self.metadata = []
                                
                                for doc_id, document in documents.items():
                                    clean_item = {
                                        'id': doc_id,
                                        'content': getattr(document, 'page_content', ''),
                                        'metadata': getattr(document, 'metadata', {}),
                                        'name': getattr(document, 'metadata', {}).get('name', ''),
                                        'full_text': getattr(document, 'page_content', '')
                                    }
                                    self.metadata.append(clean_item)
                                
                                print(f"âœ… ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(self.metadata)}ê°œ í•­ëª©")
                                return
                                
                    except Exception as original_error:
                        print(f"âš ï¸ ì›ë³¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {original_error}")
                
                # ëª¨ë“  ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ SQLite í´ë°±
                print("âš ï¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, SQLite í´ë°± ì‚¬ìš©")
                self.metadata = None
                    
            else:
                print("âš ï¸ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            self.index = None
            self.metadata = None
    
    def get_text_embedding(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # Amazon Titan Embeddings ì‚¬ìš©
            response = self.bedrock.invoke_model(
                modelId="amazon.titan-embed-text-v1",
                body=json.dumps({
                    "inputText": text
                })
            )
            
            response_body = json.loads(response['body'].read())
            embedding = np.array(response_body['embedding'], dtype=np.float32)
            
            # FAISS ì¸ë±ìŠ¤ ì°¨ì›ì— ë§ê²Œ ì¡°ì • (768ì°¨ì›)
            if len(embedding) > 768:
                # 1536ì°¨ì›ì„ 768ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (ì•ìª½ 768ê°œë§Œ ì‚¬ìš©)
                embedding = embedding[:768]
            elif len(embedding) < 768:
                # ë¶€ì¡±í•œ ì°¨ì›ì€ 0ìœ¼ë¡œ íŒ¨ë”©
                padding = np.zeros(768 - len(embedding), dtype=np.float32)
                embedding = np.concatenate([embedding, padding])
            
            return embedding
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì„ë² ë”© ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë²¡í„° ë°˜í™˜ (768ì°¨ì›ì— ë§ì¶¤)
            return np.random.rand(768).astype(np.float32)
    
    def search_similar_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        # FAISS ì¸ë±ìŠ¤ê°€ ìˆì§€ë§Œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° SQLite í´ë°± ì‚¬ìš©
        if not FAISS_AVAILABLE or self.index is None or self.metadata is None:
            return self._fallback_search(query, top_k)
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.get_text_embedding(query)
            query_embedding = query_embedding.reshape(1, -1)
            
            # FAISS ê²€ìƒ‰
            scores, indices = self.index.search(query_embedding, top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.metadata):
                    doc = self.metadata[idx].copy()
                    doc['similarity_score'] = float(score)
                    doc['rank'] = i + 1
                    results.append(doc)
            
            return results
            
        except Exception as e:
            print(f"âŒ FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return self._fallback_search(query, top_k)
    
    def _fallback_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """FAISSê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ SQLite ê¸°ë°˜ í´ë°± ê²€ìƒ‰"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ (ì‹¤ì œ í…Œì´ë¸” êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            keywords = query.split()
            where_conditions = []
            params = []
            
            for keyword in keywords:
                where_conditions.append("(name LIKE ? OR effect LIKE ? OR full_text LIKE ?)")
                params.extend([f"%{keyword}%", f"%{keyword}%", f"%{keyword}%"])
            
            where_clause = " OR ".join(where_conditions) if where_conditions else "1=1"
            
            cursor.execute(f"""
                SELECT name, company, effect, full_text
                FROM drugs 
                WHERE {where_clause}
                LIMIT ?
            """, params + [top_k])
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'name': row[0] or '',
                    'company': row[1] or '',
                    'effect': row[2] or '',
                    'full_text': row[3] or '',
                    'similarity_score': 0.8,  # ì„ì˜ ì ìˆ˜
                    'rank': len(results) + 1,
                    'source': 'sqlite_fallback'
                })
            
            conn.close()
            return results
            
        except Exception as e:
            print(f"âŒ í´ë°± ê²€ìƒ‰ë„ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_supplement_interactions(self, supplement_names: List[str]) -> List[Dict[str, Any]]:
        """ì˜ì–‘ì œ ê°„ ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        interactions = []
        
        for supplement in supplement_names:
            query = f"{supplement} ìƒí˜¸ì‘ìš© ë¶€ì‘ìš© ì£¼ì˜ì‚¬í•­"
            results = self.search_similar_documents(query, top_k=3)
            
            for result in results:
                if result.get('full_text'):
                    interactions.append({
                        'supplement': supplement,
                        'interaction_info': result['full_text'][:200] + "...",  # ì²˜ìŒ 200ìë§Œ
                        'effect': result.get('effect', ''),
                        'confidence': result.get('similarity_score', 0.5)
                    })
        
        return interactions
    
    def get_context_for_recommendation(self, user_info: Dict[str, Any], health_concerns: List[str]) -> str:
        """ì˜ì–‘ì œ ì¶”ì²œì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        context_parts = []
        
        # ê±´ê°• ê´€ì‹¬ì‚¬ë³„ ê²€ìƒ‰
        for concern in health_concerns:
            query = f"{concern} ì˜ì–‘ì œ ì¶”ì²œ íš¨ê³¼"
            results = self.search_similar_documents(query, top_k=2)
            
            for result in results:
                context_parts.append(f"[{concern} ê´€ë ¨] {result.get('name', '')}: {result.get('effect', '')}")
        
        # ë‚˜ì´ëŒ€ë³„ ì¶”ì²œ
        age = user_info.get('age', 65)
        if age >= 65:
            query = "ì‹œë‹ˆì–´ ë…¸ì¸ ì˜ì–‘ì œ ì¶”ì²œ"
        elif age >= 50:
            query = "ì¤‘ë…„ ì˜ì–‘ì œ ì¶”ì²œ"
        else:
            query = "ì„±ì¸ ì˜ì–‘ì œ ì¶”ì²œ"
            
        age_results = self.search_similar_documents(query, top_k=3)
        for result in age_results:
            context_parts.append(f"[ì—°ë ¹ëŒ€ ì¶”ì²œ] {result.get('name', '')}: {result.get('effect', '')}")
        
        # ì„±ë³„ë³„ ì¶”ì²œ
        gender = user_info.get('gender', '')
        if gender in ['ì—¬ì„±', 'female']:
            query = "ì—¬ì„± ì˜ì–‘ì œ ì¶”ì²œ"
            gender_results = self.search_similar_documents(query, top_k=2)
            for result in gender_results:
                context_parts.append(f"[ì—¬ì„± ì¶”ì²œ] {result.get('name', '')}: {result.get('effect', '')}")
        
        return "\n".join(context_parts[:10])  # ìµœëŒ€ 10ê°œ ì»¨í…ìŠ¤íŠ¸
    
    def get_safety_information(self, supplements: List[str]) -> str:
        """ì˜ì–‘ì œ ì•ˆì „ì„± ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        safety_info = []
        
        for supplement in supplements:
            query = f"{supplement} ì•ˆì „ì„± ë¶€ì‘ìš© ì£¼ì˜ì‚¬í•­ ê¸ˆê¸°"
            results = self.search_similar_documents(query, top_k=2)
            
            for result in results:
                if result.get('full_text'):
                    safety_info.append(f"{supplement}: {result.get('full_text', '')[:100]}...")  # ì²˜ìŒ 100ìë§Œ
        
        return "\n".join(safety_info)

# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = None

def get_rag_system():
    """RAG ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global rag_system
    if rag_system is None:
        rag_system = RAGSystem()
    return rag_system

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    rag = RAGSystem()
    
    print("ğŸ§ª RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query = "ë¹„íƒ€ë¯¼ D íš¨ê³¼"
    results = rag.search_similar_documents(query, top_k=3)
    
    print(f"ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    
    for i, result in enumerate(results, 1):
        print(f"\n{i}. {result.get('name', 'Unknown')}")
        print(f"   ìœ ì‚¬ë„: {result.get('similarity_score', 0):.3f}")
        print(f"   ì„¤ëª…: {result.get('description', 'N/A')[:100]}...")
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    user_info = {'age': 70, 'gender': 'ì—¬ì„±'}
    health_concerns = ['í˜ˆì••', 'ê³¨ë‹¤ê³µì¦']
    context = rag.get_context_for_recommendation(user_info, health_concerns)
    
    print(f"\nì»¨í…ìŠ¤íŠ¸ ìƒì„± ê²°ê³¼:")
    print(context[:300] + "..." if len(context) > 300 else context)