#!/usr/bin/env python3
import pickle
import os
import sys

def analyze_pickle_file():
    """pickle íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  langchain ì˜ì¡´ì„±ì„ ì œê±°í•©ë‹ˆë‹¤."""
    
    pkl_path = "../data/mfds_faiss_index/index.pkl"
    
    print("ğŸ” ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¶„ì„ ì¤‘...")
    print("=" * 50)
    
    try:
        # ì›ë³¸ íŒŒì¼ ì½ê¸° ì‹œë„
        with open(pkl_path, 'rb') as f:
            # pickle íŒŒì¼ì˜ ë‚´ìš©ì„ ë°”ì´íŠ¸ë¡œ ì½ê¸°
            content = f.read()
            print(f"íŒŒì¼ í¬ê¸°: {len(content)} bytes")
            
            # íŒŒì¼ ë‚´ìš©ì—ì„œ langchain ê´€ë ¨ ë¬¸ìì—´ í™•ì¸
            content_str = str(content)
            if 'langchain' in content_str:
                print("âš ï¸ langchain ì˜ì¡´ì„± ë°œê²¬ë¨")
            
            # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë¡œë“œ ì‹œë„
            f.seek(0)
            
            # ë‹¨ê³„ë³„ë¡œ unpickle ì‹œë„
            try:
                # ê¸°ë³¸ pickle ë¡œë“œ
                data = pickle.load(f)
                print(f"âœ… ê¸°ë³¸ pickle ë¡œë“œ ì„±ê³µ!")
                print(f"ë°ì´í„° íƒ€ì…: {type(data)}")
                
                if hasattr(data, '__len__'):
                    print(f"ë°ì´í„° ê¸¸ì´: {len(data)}")
                
                # ì²« ë²ˆì§¸ í•­ëª© ë¶„ì„
                if isinstance(data, (list, tuple)) and len(data) > 0:
                    first_item = data[0]
                    print(f"ì²« ë²ˆì§¸ í•­ëª© íƒ€ì…: {type(first_item)}")
                    
                    if hasattr(first_item, '__dict__'):
                        print(f"ì²« ë²ˆì§¸ í•­ëª© ì†ì„±: {list(first_item.__dict__.keys())}")
                    elif isinstance(first_item, dict):
                        print(f"ì²« ë²ˆì§¸ í•­ëª© í‚¤: {list(first_item.keys())}")
                        
                return data
                
            except Exception as load_error:
                print(f"âŒ ê¸°ë³¸ ë¡œë“œ ì‹¤íŒ¨: {load_error}")
                
                # ëŒ€ì•ˆ ë°©ë²•ë“¤ ì‹œë„
                try:
                    # ë‹¤ë¥¸ í”„ë¡œí† ì½œë¡œ ì‹œë„
                    f.seek(0)
                    data = pickle.load(f)
                    return data
                except:
                    pass
                
                return None
                
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def create_clean_metadata(original_data):
    """langchain ì˜ì¡´ì„± ì—†ëŠ” ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    if original_data is None:
        print("âŒ ì›ë³¸ ë°ì´í„°ê°€ ì—†ì–´ ë©”íƒ€ë°ì´í„° ìƒì„± ë¶ˆê°€")
        return None
    
    print("\nğŸ”§ ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘...")
    print("=" * 50)
    
    clean_metadata = []
    
    try:
        # langchainì˜ InMemoryDocstoreì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
        if hasattr(original_data, '__len__') and len(original_data) >= 1:
            docstore = original_data[0]  # ì²« ë²ˆì§¸ í•­ëª©ì´ docstore
            
            if hasattr(docstore, '_dict'):
                documents = docstore._dict
                print(f"ë¬¸ì„œ ì €ì¥ì†Œì—ì„œ {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
                
                for doc_id, document in documents.items():
                    clean_item = {
                        'id': doc_id,
                        'content': '',
                        'metadata': {}
                    }
                    
                    # langchain Document ê°ì²´ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    if hasattr(document, 'page_content'):
                        clean_item['content'] = document.page_content
                    
                    if hasattr(document, 'metadata'):
                        clean_item['metadata'] = document.metadata
                        
                        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì£¼ìš” í•„ë“œ ì¶”ì¶œ
                        metadata = document.metadata
                        clean_item['name'] = metadata.get('name', '')
                        clean_item['company'] = metadata.get('company', '')
                        clean_item['effect'] = metadata.get('effect', '')
                        clean_item['full_text'] = metadata.get('full_text', document.page_content)
                    
                    clean_metadata.append(clean_item)
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    if len(clean_metadata) % 500 == 0:
                        print(f"ì²˜ë¦¬ ì¤‘... {len(clean_metadata)}/{len(documents)}")
        
        print(f"âœ… ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(clean_metadata)}ê°œ í•­ëª©")
        
        # ìƒˆ íŒŒì¼ë¡œ ì €ì¥
        clean_pkl_path = "../data/mfds_faiss_index/index_clean.pkl"
        with open(clean_pkl_path, 'wb') as f:
            pickle.dump(clean_metadata, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        print(f"âœ… ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {clean_pkl_path}")
        
        return clean_metadata
        
    except Exception as e:
        print(f"âŒ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_clean_metadata():
    """ìƒì„±ëœ ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„°ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    clean_pkl_path = "../data/mfds_faiss_index/index_clean.pkl"
    
    if not os.path.exists(clean_pkl_path):
        print("âŒ ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    print("\nğŸ§ª ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        with open(clean_pkl_path, 'rb') as f:
            clean_data = pickle.load(f)
        
        print(f"âœ… ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
        print(f"ë°ì´í„° íƒ€ì…: {type(clean_data)}")
        print(f"ë°ì´í„° ê¸¸ì´: {len(clean_data)}")
        
        # ì²« ë²ˆì§¸ í•­ëª© í™•ì¸
        if len(clean_data) > 0:
            first_item = clean_data[0]
            print(f"ì²« ë²ˆì§¸ í•­ëª©: {first_item}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    # 1. ì›ë³¸ ë©”íƒ€ë°ì´í„° ë¶„ì„
    original_data = analyze_pickle_file()
    
    # 2. ê¹¨ë—í•œ ë©”íƒ€ë°ì´í„° ìƒì„±
    if original_data:
        clean_data = create_clean_metadata(original_data)
        
        # 3. í…ŒìŠ¤íŠ¸
        if clean_data:
            test_clean_metadata()
    else:
        print("âŒ ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ langchain_communityë¥¼ ì„¤ì¹˜í•´ë³´ì„¸ìš”: pip install langchain-community")